{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio\n",
    "from transformers import BertTokenizer\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MELDDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None, max_video_len=30, max_audio_len=16000, max_text_len=128):\n",
    "        self.df = pd.read_csv(csv)\n",
    "        self.label = {self.df['Emotion'].unique()[i]: i for i in range(len(self.df['Emotion'].unique()))}\n",
    "        self.path = path\n",
    "        self.max_video_len = max_video_len\n",
    "        self.max_audio_len = max_audio_len\n",
    "        self.max_text_len = max_text_len\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.video_transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def load_video(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = self.video_transform(frame)\n",
    "            frames.append(frame)\n",
    "            if len(frames) >= self.max_video_len:\n",
    "                break\n",
    "        cap.release()\n",
    "        if len(frames) < self.max_video_len:\n",
    "            return frames[-1]\n",
    "        else:\n",
    "            return frames[self.max_video_len - 1]\n",
    "\n",
    "    def load_audio(self, audio_path):\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            if waveform.size(0) > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            if waveform.size(1) > self.max_audio_len:\n",
    "                waveform = waveform[:, :self.max_audio_len]\n",
    "            else:\n",
    "                padding = self.max_audio_len - waveform.size(1)\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "            return waveform\n",
    "        except:\n",
    "            return torch.zeros(1, self.max_audio_len)\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_text_len, return_tensors='pt')\n",
    "        return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = 'dia' + str(self.df.iloc[idx]['Dialogue_ID']) + '_utt' + str(self.df.iloc[idx]['Utterance_ID']) + '.mp4'\n",
    "        text = self.df.iloc[idx]['Utterance'].replace('\\x92', \"'\")\n",
    "\n",
    "        video = self.load_video(self.path + filename)\n",
    "        audio = self.load_audio(self.path + filename)\n",
    "        text, attention_mask = self.tokenize_text(text)\n",
    "        label = self.label[self.df.iloc[idx]['Emotion']]\n",
    "        return video, audio, text, attention_mask, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    videos, audios, texts, attention_masks, labels = zip(*batch)\n",
    "    videos = torch.stack(videos)\n",
    "    audios = torch.stack(audios)\n",
    "    texts = torch.stack(texts)\n",
    "    attention_masks = torch.stack(attention_masks)\n",
    "    labels = torch.tensor(labels)\n",
    "    return texts, attention_masks, audios, videos, labels\n",
    "\n",
    "def MELD(datatype, transform=None, batch_size=4, collate=collate_fn):\n",
    "    \"\"\"DataLoader. \\\\\n",
    "    Expected File structure is: \\\\\n",
    "    ├── train\\\\\n",
    "    ├── valid\\\\\n",
    "    ├── test  \\\\\n",
    "    ├── train.csv\\\\\n",
    "    ├── valid.csv\\\\\n",
    "    └── test.csv\\\\\n",
    "    Change if you want. \\\\\n",
    "    If transform is None, it just resizes data and returns Tensor.\\\\\n",
    "    Video (Batch, Frame, Channel, Height, Width) \\\\\n",
    "    Audio (Batch, Channel, Sample) \\\\\n",
    "    Text  (Batch, tokenized Length)\\\\\n",
    "    Label (Batch)\n",
    "    \"\"\"\n",
    "    # Data to load\n",
    "    if datatype == 'train':\n",
    "        csv_file = '/content/drive/MyDrive/MELD/train_sent_emo.csv'\n",
    "        data_folder = '/content/drive/MyDrive/MELD/train_splits/'\n",
    "    elif datatype == 'valid':\n",
    "        csv_file = '/content/drive/MyDrive/MELD/MELD.Raw/dev_sent_emo.csv'\n",
    "        data_folder = '/content/drive/MyDrive/MELD/dev_splits_complete/'\n",
    "    elif datatype == 'test':\n",
    "        csv_file = '/content/drive/MyDrive/MELD/MELD.Raw/test_sent_emo.csv'\n",
    "        data_folder = '/content/drive/MyDrive/MELD/output_repeated_splits_test/'\n",
    "    # transform\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((480, 640)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    # Load data\n",
    "    dataset = MELDDataset(csv_file, data_folder, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "    return dataloader\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataloader = MELD('test', batch_size=16)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_input_tokens, batch_attention_masks, batch_audio, batch_video, batch_labels = batch\n",
    "        # Display one image from the batch for verification\n",
    "        img = batch_video[0].permute(1, 2, 0).numpy()*255\n",
    "        cv2_imshow(img)\n",
    "        break\n",
    "!git clone https://github.com/jaeyeonkim99/EnCLAP.git\n",
    "%cd EnCLAP/\n",
    "!wget https://huggingface.co/lukewys/laion_clap/resolve/main/630k-audioset-best.pt?download=true\n",
    "dataloader = MELD('test', batch_size=16)\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch_input_tokens, batch_attention_masks, batch_audio, batch_video, batch_labels = batch\n",
    "    !python inference.py  --ckpt /content/drive/MyDrive/Audio captioning --clap_ckpt /content/drive/MyDrive/EnCLAP/630k-audioset-best.pt --input batch_audio\n",
    "!pip install laion_clap\n",
    "!pip install encodec\n",
    "# audio\n",
    "from typing import Any, Dict\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from encodec import EncodecModel\n",
    "from encodec.utils import convert_audio\n",
    "from laion_clap import CLAP_Module\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from modeling.enclap_bart import EnClapBartConfig, EnClapBartForConditionalGeneration\n",
    "\n",
    "\n",
    "class EnClap:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ckpt_path: str,\n",
    "        clap_audio_model: str = \"HTSAT-tiny\",\n",
    "        clap_enable_fusion: bool = True,\n",
    "        clap_ckpt_path: str = None,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        config = EnClapBartConfig.from_pretrained(ckpt_path)\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "        self.model = (\n",
    "            EnClapBartForConditionalGeneration.from_pretrained(ckpt_path)\n",
    "            .to(self.device)\n",
    "            .eval()\n",
    "        )\n",
    "\n",
    "        self.encodec = EncodecModel.encodec_model_24khz().to(self.device)\n",
    "        self.encodec.set_target_bandwidth(12.0)\n",
    "        self.clap_model = CLAP_Module(enable_fusion=clap_enable_fusion, amodel=clap_audio_model, device=self.device)\n",
    "        self.clap_model.load_ckpt(clap_ckpt_path)\n",
    "\n",
    "        self.generation_config = {\n",
    "            \"_from_model_config\": True,\n",
    "            \"bos_token_id\": 0,\n",
    "            \"decoder_start_token_id\": 2,\n",
    "            \"early_stopping\": True,\n",
    "            \"eos_token_id\": 2,\n",
    "            \"forced_bos_token_id\": 0,\n",
    "            \"forced_eos_token_id\": 2,\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"num_beams\": 4,\n",
    "            \"pad_token_id\": 1,\n",
    "            \"max_length\": 50,\n",
    "        }\n",
    "        self.max_seq_len = config.max_position_embeddings - 3\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer_from_audio_file(\n",
    "        self, audio_file: str, generation_config: Dict[str, Any] = None\n",
    "    ) -> str:\n",
    "        if generation_config is None:\n",
    "            generation_config = self.generation_config\n",
    "        audio, res = torchaudio.load(audio_file)\n",
    "        return self.infer_from_audio(audio[0], res)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer_from_audio(\n",
    "        self, audio: torch.Tensor, res: int, generation_config: Dict[str, Any] = None\n",
    "    ) -> str:\n",
    "        if generation_config is None:\n",
    "            generation_config = self.generation_config\n",
    "        if audio.dtype == torch.short:\n",
    "            audio = audio / 2**15\n",
    "        if audio.dtype == torch.int:\n",
    "            audio = audio / 2**31\n",
    "        encodec_audio = (\n",
    "            convert_audio(\n",
    "                audio.unsqueeze(0), res, self.encodec.sample_rate, self.encodec.channels\n",
    "            )\n",
    "            .unsqueeze(0)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        encodec_frames = self.encodec.encode(encodec_audio)\n",
    "        encodec_frames = torch.cat(\n",
    "            [codebook for codebook, _ in encodec_frames], dim=-1\n",
    "        ).mT\n",
    "\n",
    "        clap_audio = torchaudio.transforms.Resample(res, 48000)(audio).unsqueeze(0)\n",
    "        clap_embedding = self.clap_model.get_audio_embedding_from_data(clap_audio, use_tensor=True)\n",
    "\n",
    "        return self._infer(encodec_frames, clap_embedding, generation_config)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _infer(\n",
    "        self,\n",
    "        encodec_frames: torch.LongTensor,\n",
    "        clap_embedding: torch.Tensor,\n",
    "        generation_config: Dict[str, Any] = None,\n",
    "    ) -> str:\n",
    "        input_ids = torch.cat(\n",
    "            [\n",
    "                torch.ones(\n",
    "                    (encodec_frames.shape[0], 2, encodec_frames.shape[-1]),\n",
    "                    dtype=torch.long,\n",
    "                ).to(self.device)\n",
    "                * self.tokenizer.bos_token_id,\n",
    "                encodec_frames[:, : self.max_seq_len],\n",
    "                torch.ones(\n",
    "                    (encodec_frames.shape[0], 1, encodec_frames.shape[-1]),\n",
    "                    dtype=torch.long,\n",
    "                ).to(self.device)\n",
    "                * self.tokenizer.eos_token_id,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        encodec_mask = torch.LongTensor(\n",
    "            [[0, 0] + [1] * (input_ids.shape[1] - 3) + [0]]\n",
    "        ).to(self.device)\n",
    "\n",
    "        enclap_bart_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"encodec_mask\": encodec_mask,\n",
    "            \"clap_embedding\": clap_embedding,\n",
    "        }\n",
    "\n",
    "        results = self.model.generate(**enclap_bart_inputs, **generation_config)\n",
    "        caption = self.tokenizer.batch_decode(results, skip_special_tokens=True)\n",
    "\n",
    "        return caption\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer_from_encodec(\n",
    "        self,\n",
    "        encodec_path,\n",
    "        clap_path,\n",
    "        generation_config: Dict[str, Any] = None,\n",
    "    ):\n",
    "        if generation_config is None:\n",
    "            generation_config = self.generation_config\n",
    "        encodec_frames = torch.from_numpy(np.load(encodec_path)).unsqueeze(0).cuda()\n",
    "        clap_embedding = torch.from_numpy(np.load(clap_path)).unsqueeze(0).cuda()\n",
    "\n",
    "        return self._infer(encodec_frames, clap_embedding, generation_config)\n",
    "enclap = EnClap(\n",
    "        ckpt_path='/content/drive/MyDrive/Audio_captioning',\n",
    "        clap_ckpt_path='/content/drive/MyDrive/EnCLAP/630k-audioset-fusion-best.pt',\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    "\n",
    "predict = enclap.infer_from_audio_file('/content/drive/MyDrive/final_videos_testdia101_utt2.wav')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
